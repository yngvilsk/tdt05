{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializing the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "NEUTRAL_DATASET_PATH = '/content/datasets/training.1600000.processed.noemoticon.csv'\n",
    "RADICAL_DATASET_PATH =  '/content/datasets/tweets.csv'\n",
    "TRAIN_SIZE = 0.8\n",
    "\n",
    "# Text Cleaning\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loadind the data and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_radical = pd.read_csv(RADICAL_DATASET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only the column containing the tweets\n",
    "df_radical = df_radical.filter(['tweets'], axis=1)\n",
    "df_radical['is_radical'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_non_radical = pd.read_csv(NEUTRAL_DATASET_PATH, encoding='ISO-8859-1', names=['target', 'ids', 'date', 'flag', 'user', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a neutral data set containing 50/50 positive and negative tweets\n",
    "negative_tweets = df_non_radical[df_non_radical['target'] == 0].sample(75000)\n",
    "\n",
    "positive_tweets = df_non_radical[df_non_radical['target'] == 4].sample(75000)\n",
    "\n",
    "df_neutral = pd.concat([negative_tweets, positive_tweets])\n",
    "\n",
    "# Keep only the column containing the tweets\n",
    "df_neutral = df_neutral.filter(['text'], axis=1)\n",
    "\n",
    "# Rename 'text' column to 'tweets'\n",
    "df_neutral.rename(columns={'text': 'tweets'}, inplace=True)\n",
    "df_neutral['is_radical'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(tweet):\n",
    "    # Many tweets from radical dataset starts with 'ENGLISHT RANSLATION:', remove this\n",
    "    tweet = re.sub(r'ENGLISH TRANSLATION:','', tweet)\n",
    "\n",
    "    # Remove link, user and special characters\n",
    "    tweet = re.sub('@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+', ' ', str(tweet).lower()).strip()\n",
    "    tokens = []\n",
    "\n",
    "    # Remove stopwords and stem words\n",
    "    for token in tweet.split():\n",
    "        if token and token not in stopwords.words(\"english\"):\n",
    "              tokens.append(stemmer.stem(token))\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the column containing the tweets\n",
    "df_radical['tweets'] = df_radical['tweets'].apply(preprocess)\n",
    "df_neutral['tweets'] = df_neutral['tweets'].apply(preprocess)\n",
    "\n",
    "# Combine the datasets\n",
    "df_all_tweets = pd.concat([df_radical, df_neutral])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_data, test_data = train_test_split(df_all_tweets, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Word2Vec Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the wrd2vec model\n",
    "w2v_model = Word2Vec(vector_size=50, window=5, min_count=7, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the tweets so they are on the right format\n",
    "documents = train_data['tweets'].apply(gensim.utils.simple_preprocess)\n",
    "\n",
    "# Build the vocabulary\n",
    "w2v_model.build_vocab(documents)\n",
    "vocabulary = w2v_model.wv.index_to_key\n",
    "\n",
    "# Train the word2vec model\n",
    "w2v_model.train(documents, total_examples=len(documents), epochs=200)\n",
    "w2v_model.save('w2v.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the word embeddings using common examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.wv.most_similar(\"love\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.wv.most_similar(\"hate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.wv.most_similar(\"islam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = w2v_model.wv['king'] - w2v_model.wv['man'] + w2v_model.wv['woman']\n",
    "w2v_model.wv.most_similar([vec])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a word vector for each tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the tweets to vectors by calculating the average word vector of all the words in the tweet. \n",
    "def tweet_to_vector(tweet, model, vocabulary):\n",
    "  # Adjust the size to the model's vector size\n",
    "  vectorized_tweet = np.zeros(model.vector_size)\n",
    "\n",
    "  # Convert the tweets to a vector of the words\n",
    "  words = tweet.split()\n",
    "  word_count = 0\n",
    "\n",
    "  # Calculate how many words in the tweet are in the vocabulary\n",
    "  for word in words:\n",
    "    if word in vocabulary:\n",
    "      vectorized_tweet += model.wv[word]\n",
    "      word_count += 1\n",
    "\n",
    "  # Calculate the average vector\n",
    "  if word_count > 0:\n",
    "    vectorized_tweet /= word_count\n",
    "\n",
    "  return vectorized_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['vectors'] = train_data['tweets'].apply(lambda tweet: tweet_to_vector(tweet, w2v_model, vocabulary))\n",
    "test_data['vectors'] = test_data['tweets'].apply(lambda tweet: tweet_to_vector(tweet, w2v_model, vocabulary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating BoW Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Bag of Words model, that transform a text corpus to a matrix of counts\n",
    "bow_vectorizer = CountVectorizer(max_features=1000)\n",
    "\n",
    "# Fit the vectorizer on the training data\n",
    "bow_vectorizer.fit(train_data['tweets'])\n",
    "\n",
    "# Transform training and test data into BoW vectors\n",
    "X_train_bow = bow_vectorizer.transform(train_data['tweets']).toarray()\n",
    "X_test_bow = bow_vectorizer.transform(test_data['tweets']).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANN Model training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the final input features for the neural network\n",
    "X_train_w2v = np.vstack(train_data['vectors'].values)\n",
    "X_test_w2v = np.vstack(test_data['vectors'].values)\n",
    "y_train = train_data['is_radical'].values\n",
    "y_test = test_data['is_radical'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model architecture, with word2vec as input\n",
    "nn_model_w2v = keras.Sequential([\n",
    "    layers.Dense(32, activation='relu', input_shape=(X_train_w2v.shape[1],)),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(16, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(2, activation='softmax')\n",
    "])\n",
    "\n",
    "# Define the learning rate schedule\n",
    "initial_learning_rate = 0.001\n",
    "decay_steps = 1000\n",
    "decay_rate = 0.96\n",
    "\n",
    "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate,\n",
    "    decay_steps=decay_steps,\n",
    "    decay_rate=decay_rate,\n",
    "    staircase=True\n",
    ")\n",
    "\n",
    "# Building the model\n",
    "nn_model_w2v.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=lr_schedule),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model architecture, with BoW as input\n",
    "nn_model_bow = keras.Sequential([\n",
    "    layers.Dense(32, activation='relu', input_shape=(X_train_bow.shape[1],)),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(16, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(2, activation='softmax')\n",
    "])\n",
    "\n",
    "# Building the model\n",
    "nn_model_bow.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=lr_schedule),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_w2v = nn_model_w2v.fit(\n",
    "    X_train_w2v,\n",
    "    y_train,\n",
    "    epochs=15,\n",
    "    validation_data=(X_test_w2v, y_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_bow = nn_model_bow.fit(\n",
    "    X_train_bow,\n",
    "    y_train,\n",
    "    epochs=15,\n",
    "    validation_data=(X_test_bow, y_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANN Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model_w2v.summary()\n",
    "nn_model_bow.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the ANN with Word2Vec model\n",
    "test_loss_w2v, test_accuracy_w2v = nn_model_w2v.evaluate(X_test_w2v, y_test, verbose=2)\n",
    "print(\"ANN with Word2Vec\")\n",
    "print(f\"Test accuracy: {test_accuracy_w2v}\")\n",
    "print(f\"Test loss: {test_loss_w2v}\")\n",
    "\n",
    "# Evaluate the ANN with BoW model\n",
    "test_loss_bow, test_accuracy_bow = nn_model_bow.evaluate(X_test_bow, y_test, verbose=2)\n",
    "print(\"ANN with BoW\")\n",
    "print(f\"Test accuracy: {test_accuracy_bow}\")\n",
    "print(f\"Test loss: {test_loss_bow}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation accuracy values\n",
    "\n",
    "accuracy_ylim = (0.955, 0.985)\n",
    "loss_ylim = (0.06, 0.15)\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.suptitle('ANN with Word2Vec Model Training and Validation Metrics')\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history_w2v.history['accuracy'])\n",
    "plt.plot(history_w2v.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylim(accuracy_ylim)\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history_w2v.history['loss'])\n",
    "plt.plot(history_w2v.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.ylim(loss_ylim)\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.suptitle('ANN with BoW Model Training and Validation Metrics')\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history_bow.history['accuracy'])\n",
    "plt.plot(history_bow.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim(accuracy_ylim)\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history_bow.history['loss'])\n",
    "plt.plot(history_bow.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.ylim(loss_ylim)\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Confusin Matrix for the W2V model\n",
    "predictions_w2v = nn_model_w2v.predict(X_test_w2v)\n",
    "predicted_classes_w2v = np.argmax(predictions_w2v, axis=1)\n",
    "\n",
    "# Generate the confusion matrix\n",
    "cm_w2v = confusion_matrix(y_test, predicted_classes_w2v)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "sns.heatmap(cm_w2v, annot=True, fmt='d', ax=ax, cmap=plt.cm.Blues, cbar=False)\n",
    "ax.set(xlabel=\"Predicted Label\", ylabel=\"True Label\", title=\"ANN with Word2Vec Confusion Matrix\")\n",
    "plt.yticks(rotation=0)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Create Confusion Matrix for the BoW model\n",
    "predictions_bow = nn_model_bow.predict(X_test_bow)\n",
    "predicted_classes_bow = np.argmax(predictions_bow, axis=1)\n",
    "\n",
    "cm_bow = confusion_matrix(y_test, predicted_classes_bow)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "sns.heatmap(cm_bow, annot=True, fmt='d', ax=ax, cmap=plt.cm.Blues, cbar=False)\n",
    "ax.set(xlabel=\"Predicted Label\", ylabel=\"True Label\", title=\"ANNN with BoW Confusion Matrix\")\n",
    "plt.yticks(rotation=0)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_w2v = precision_score(y_test, predicted_classes_w2v, average='binary')\n",
    "recall_w2v = recall_score(y_test, predicted_classes_w2v, average='binary')\n",
    "f1_w2v = f1_score(y_test, predicted_classes_w2v, average='binary')\n",
    "\n",
    "print(\"ANN with Word2Vec:\")\n",
    "print(f\"Precision: {precision_w2v}\")\n",
    "print(f\"Recall: {recall_w2v}\")\n",
    "print(f\"F1 Score: {f1_w2v}\")\n",
    "\n",
    "precision_bow = precision_score(y_test, predicted_classes_bow, average='binary')\n",
    "recall_bow = recall_score(y_test, predicted_classes_bow, average='binary')\n",
    "f1_bow = f1_score(y_test, predicted_classes_bow, average='binary')\n",
    "\n",
    "print(\"ANN with BoW:\")\n",
    "print(f\"Precision: {precision_bow}\")\n",
    "print(f\"Recall: {recall_bow}\")\n",
    "print(f\"F1 Score: {f1_bow}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model = DummyClassifier(strategy='stratified', random_state=42)\n",
    "\n",
    "baseline_model.fit(X_train_w2v, y_train)\n",
    "\n",
    "baseline_predictions = baseline_model.predict(X_test_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "cm_baseline = confusion_matrix(y_test, baseline_predictions)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "sns.heatmap(cm_baseline, annot=True, fmt='d', cmap=plt.cm.Blues, cbar=False)\n",
    "plt.title('Baseline Classifier Confusion Matrix')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_accuracy = accuracy_score(y_test, baseline_predictions)\n",
    "print(f\"Baseline Classifier Accuracy: {baseline_accuracy:.2f}\")\n",
    "\n",
    "baseline_precision = precision_score(y_test, baseline_predictions, average='binary')\n",
    "baseline_recall = recall_score(y_test, baseline_predictions, average='binary')\n",
    "baseline_f1 = f1_score(y_test, baseline_predictions, average='binary')\n",
    "\n",
    "print(\"Baseline:\")\n",
    "print(f\"Precision: {baseline_precision}\")\n",
    "print(f\"Recall: {baseline_recall}\")\n",
    "print(f\"F1 Score: {baseline_f1}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
